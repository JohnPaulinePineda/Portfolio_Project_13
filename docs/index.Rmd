---
title: "R : Remedial Procedures in Handling Imbalanced Data for Classification"
author: "John Pauline Pineda"
date: "December 12, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document presents a non-exhaustive list of remedial procedures applied for severe class imbalance using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>.
|
| Data resampling and synthetic data generation address imbalanced classification problems by changing the data set used for model training to have more balanced data. In data resampling, copies of instances from the under-represented class can be added (called oversampling), or instances from the over-represented class can be deleted (called undersampling), to even-up the classes. In synthetic data generation, new instances are created from the minor class instead of merely creating copies, by selecting two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**caret**</mark>, <mark style="background-color: #CCECFF">**themis**</mark> and <mark style="background-color: #CCECFF">**ROSE**</mark> packages attempt to augment imbalanced data prior to model training by updating the original data set to minimize the effect of the disproportionate ratio of instances in each class.
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**Sonar**</mark>  dataset from the  <mark style="background-color: #CCECFF">**mlbench**</mark> package was used for this illustrated example. The original dataset was transformed to simulate class imbalance.
|
| Preliminary dataset assessment:
|
| **[A]** 136 rows (observations)
|      **[A.1]** Train Set = 96 observations with class ratio of 80:20
|      **[A.2]** Test Set = 40 observations with class ratio of 80:20
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(mlbench)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)
library(ROSE)

##################################
# Loading source and
# formulating the train set
##################################
data(Sonar)

Sonar.Original <- Sonar

Sonar.M <- Sonar[Sonar$Class=="M",]
Sonar.R <- Sonar[Sonar$Class=="R",]
set.seed(12345678)
Sonar.R.Reduced <- Sonar.R[sample(1:nrow(Sonar.R),25),]

Sonar <- as.data.frame(rbind(Sonar.M,Sonar.R.Reduced))

set.seed(12345678)
Sonar_Partition <- createDataPartition(Sonar$Class, p = .70, list = FALSE)
Sonar_Train <- Sonar[Sonar_Partition,]
Sonar_Test  <- Sonar[-Sonar_Partition,]

##################################
# Performing a general exploration of the train set
##################################
dim(Sonar_Train)
str(Sonar_Train)
summary(Sonar_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Sonar_Test)
str(Sonar_Test)
summary(Sonar_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Sonar_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```
##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 17 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[B.2]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[B.3]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[B.4]** <span style="color: #FF0000">V16</span> variable (numeric)
|      **[B.5]** <span style="color: #FF0000">V19</span> variable (numeric)
|      **[B.6]** <span style="color: #FF0000">V26</span> variable (numeric)
|      **[B.7]** <span style="color: #FF0000">V28</span> variable (numeric)
|      **[B.8]** <span style="color: #FF0000">V32</span> variable (numeric)
|      **[B.9]** <span style="color: #FF0000">V34</span> variable (numeric)
|      **[B.10]** <span style="color: #FF0000">V35</span> variable (numeric)
|      **[B.11]** <span style="color: #FF0000">V37</span> variable (numeric)
|      **[B.12]** <span style="color: #FF0000">V38</span> variable (numeric)
|      **[B.13]** <span style="color: #FF0000">V41</span> variable (numeric)
|      **[B.14]** <span style="color: #FF0000">V42</span> variable (numeric)
|      **[B.15]** <span style="color: #FF0000">V43</span> variable (numeric)
|      **[B.16]** <span style="color: #FF0000">V45</span> variable (numeric)
|      **[B.17]** <span style="color: #FF0000">V48</span> variable (numeric)
|
| **[C]** No low variance noted for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness noted for any variable with Skewness>3 or Skewness<(-3).
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Sonar_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,sapply(DQA.Predictors, is.numeric)]

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,sapply(DQA.Predictors, is.factor)]

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```
##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| Outlier data assessment:
|
| **[A]** Outliers noted for 39 variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps.
|      **[A.1]** <span style="color: #FF0000">V1</span> variable (5 outliers detected)
|      **[A.2]** <span style="color: #FF0000">V2</span> variable (6 outliers detected)
|      **[A.3]** <span style="color: #FF0000">V3</span> variable (6 outliers detected)
|      **[A.4]** <span style="color: #FF0000">V4</span> variable (5 outliers detected)
|      **[A.5]** <span style="color: #FF0000">V5</span> variable (2 outliers detected)
|      **[A.6]** <span style="color: #FF0000">V6</span> variable (4 outliers detected)
|      **[A.7]** <span style="color: #FF0000">V7</span> variable (2 outliers detected)
|      **[A.8]** <span style="color: #FF0000">V8</span> variable (6 outliers detected)
|      **[A.9]** <span style="color: #FF0000">V9</span> variable (4 outliers detected)
|      **[A.10]** <span style="color: #FF0000">V10</span> variable (4 outliers detected)
|      **[A.11]** <span style="color: #FF0000">V11</span> variable (2 outliers detected)
|      **[A.13]** <span style="color: #FF0000">V13</span> variable (1 outlier detected)
|      **[A.14]** <span style="color: #FF0000">V14</span> variable (2 outliers detected)
|      **[A.15]** <span style="color: #FF0000">V15</span> variable (2 outliers detected)
|      **[A.16]** <span style="color: #FF0000">V24</span> variable (1 outlier detected)
|      **[A.16]** <span style="color: #FF0000">V25</span> variable (3 outliers detected)
|      **[A.17]** <span style="color: #FF0000">V38</span> variable (5 outliers detected)
|      **[A.18]** <span style="color: #FF0000">V39</span> variable (1 outlier detected)
|      **[A.19]** <span style="color: #FF0000">V40</span> variable (1 outliers detected)
|      **[A.20]** <span style="color: #FF0000">V41</span> variable (1 outlier detected)
|      **[A.21]** <span style="color: #FF0000">V42</span> variable (3 outliers detected)
|      **[A.22]** <span style="color: #FF0000">V43</span> variable (1 outliers detected)
|      **[A.23]** <span style="color: #FF0000">V44</span> variable (2 outliers detected)
|      **[A.25]** <span style="color: #FF0000">V46</span> variable (8 outliers detected)
|      **[A.26]** <span style="color: #FF0000">V47</span> variable (6 outliers detected)
|      **[A.27]** <span style="color: #FF0000">V48</span> variable (6 outliers detected)
|      **[A.28]** <span style="color: #FF0000">V49</span> variable (1 outliers detected)
|      **[A.29]** <span style="color: #FF0000">V50</span> variable (4 outliers detected)
|      **[A.30]** <span style="color: #FF0000">V51</span> variable (4 outliers detected)
|      **[A.31]** <span style="color: #FF0000">V52</span> variable (5 outliers detected)
|      **[A.32]** <span style="color: #FF0000">V53</span> variable (2 outliers detected)
|      **[A.33]** <span style="color: #FF0000">V54</span> variable (4 outliers detected)
|      **[A.34]** <span style="color: #FF0000">V55</span> variable (3 outliers detected)
|      **[A.35]** <span style="color: #FF0000">V56</span> variable (2 outliers detected)
|      **[A.36]** <span style="color: #FF0000">V57</span> variable (3 outliers detected)
|      **[A.37]** <span style="color: #FF0000">V58</span> variable (6 outliers detected)
|      **[A.38]** <span style="color: #FF0000">V59</span> variable (6 outliers detected)
|      **[A.39]** <span style="color: #FF0000">V60</span> variable (3 outliers detected)
|
```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```
###  1.3.2 Zero and Near-Zero Variance
|
| Zero and near-zero variance data assessment:
|
| **[A]** Low variance noted for 17 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variables using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){

  print("No low variance predictors noted.")

} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))

  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))

  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))

  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }

  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLowVariance)
  
} 

```
###  1.3.3 Collinearity
|
| High collinearity data assessment:
|
| **[A]** No high correlation > 95% were noted for any variable as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  
  print("No highly correlated predictors noted.")
  
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))

  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05,
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))

}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedHighCorrelation)

}

```
###  1.3.4 Linear Dependencies
|
| Linear dependency data assessment:
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))

  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }

}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)

  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLinearlyDependent)

} else {
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA)
  
}

```

###  1.3.5 Shape Transformation
|
| Data transformation assessment:
|
| **[A]** A number of numeric variables in the dataset were observed to be right-skewed which required shape transformation for data distribution stability. Considering that all numeric variables were strictly positive values, the <span style="color: #0000FF">BoxCox</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was used to transform their distributional shapes.
|
```{r section_1.3.5, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Gathering descriptive statistics
##################################
(DPA_BoxCoxTransformedSkimmed <- skim(DPA_BoxCoxTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA_BoxCoxTransformed)

```

###  1.3.6 Centering and Scaling
|
| Centering and scaling data assessment:
|
| **[A]** To maintain numerical stability during modelling, centering and scaling transformations were applied on the transformed numeric variables. The <span style="color: #0000FF">center</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was implemented which subtracts the average value of a numeric variable to all the values. As a result of centering, the variables had zero mean values. In addition, the <span style="color: #0000FF">scale</span> method, also from the <mark style="background-color: #CCECFF">**caret**</mark> package, was applied which performs a center transformation with each value of the variable divided by its standard deviation. Scaling the data coerced the values to have a common standard deviation of one.
|
```{r section_1.3.6, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Applying a center and scale data transformation
##################################
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_BoxCoxTransformed, method = c("center","scale"))
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_BoxCoxTransformed)

##################################
# Gathering descriptive statistics
##################################
(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformedSkimmed <- skim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed)

```

###  1.3.7 Pre-Processed Dataset
|
| Preliminary dataset assessment:
|
| **[A]** 136 rows (observations)
|      **[A.1]** Train Set = 96 observations with class ratio of 80:20
|      **[A.2]** Test Set = 40 observations with class ratio of 80:20
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
|
| **[C]** Pre-processing actions applied:
|      **[C.1]** Centering, scaling and shape transformation applied to improve data quality
|      **[C.2]** No outlier treatment applied since the high values noted were contextually valid and sensible 
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
|
```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
Class <- DPA$Class 
PMA.Predictors.Numeric  <- DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA.Predictors.Numeric)
PMA_PreModelling_Train <- PMA_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
DPA_Test <- Sonar_Test
DPA_Test.Predictors <- DPA_Test[,!names(DPA_Test) %in% c("Class")]
DPA_Test.Predictors.Numeric <- DPA_Test.Predictors[,sapply(DPA_Test.Predictors, is.numeric)]
DPA_Test_BoxCox <- preProcess(DPA_Test.Predictors.Numeric, method = c("BoxCox"))
DPA_Test_BoxCoxTransformed <- predict(DPA_Test_BoxCox, DPA_Test.Predictors.Numeric)
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_Test_BoxCoxTransformed, method = c("center","scale"))
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_Test_BoxCoxTransformed)

##################################
# Creating the pre-modelling
# test set
##################################
Class <- DPA_Test$Class 
PMA_Test.Predictors.Numeric  <- DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_Test_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA_Test.Predictors.Numeric)
PMA_PreModelling_Test <- PMA_Test_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```
## 1.4 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Numeric variables which demonstrated differential relationships with the <span style="color: #FF0000">Class</span> response variable include:
|      **[A.1]** <span style="color: #FF0000">V1</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">V4</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">V13</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">V45</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[A.12]** <span style="color: #FF0000">V50</span> variable (numeric)
|      **[A.13]** <span style="color: #FF0000">V51</span> variable (numeric)
|
```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric, 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|")

```

## 1.5 Remedial Methods Applied for Class Imbalance using Bagged Trees

###  1.5.1 Original Data (REF)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data did not apply any class imbalance treatment for the observations.
|      **[C.1]** Train Set = 96 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:78, <span style="color: #FF0000">Class=R</span>=18)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>=7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.78151
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V22</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V7</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.64069
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
set.seed(12345678)
BTREE_REF <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                   y = PMA_PreModelling_Train$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_REF

BTREE_REF$finalModel

BTREE_REF$results

(BTREE_REF_Train_ROCCurveAUC <- BTREE_REF$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_REF_VarImp <- varImp(BTREE_REF, scale = TRUE)
plot(BTREE_REF_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (REF)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_REF_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_REF, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_REF_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_REF_Test_ROC <- roc(response = BTREE_REF_Test$BTREE_Observed,
             predictor = BTREE_REF_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_REF_Test$BTREE_Observed)))

(BTREE_REF_Test_ROCCurveAUC <- auc(BTREE_REF_Test_ROC)[1])

```

###  1.5.2 Random Undersampling - Outside Resampling (RU_OUT)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Random Undersampling - Outside Resampling](https://dl.acm.org/doi/10.1145/1007730.1007735) performs a random removal of rows for the majority class instances prior to the cross-validation process to make the occurrence of levels with the minority class equal.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random undersampling before formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 50:50 (<span style="color: #FF0000">Class=M</span>:18, <span style="color: #FF0000">Class=R</span>:18)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.82000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V35</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V21</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.90909
|
```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Conducting random undersampling
##################################
set.seed(12345678)
PMA_PreModelling_Train_RU_OUT <- downSample(x = PMA_PreModelling_Train[,-ncol(PMA_PreModelling_Train)],
                                            y = PMA_PreModelling_Train$Class)
table(PMA_PreModelling_Train_RU_OUT$Class)  

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
set.seed(12345678)
BTREE_RU_OUT <- train(x = PMA_PreModelling_Train_RU_OUT[,!names(PMA_PreModelling_Train_RU_OUT) %in% c("Class")],
                   y = PMA_PreModelling_Train_RU_OUT$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_RU_OUT

BTREE_RU_OUT$finalModel

BTREE_RU_OUT$results

(BTREE_RU_OUT_Train_ROCCurveAUC <- BTREE_RU_OUT$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_RU_OUT_VarImp <- varImp(BTREE_RU_OUT, scale = TRUE)
plot(BTREE_RU_OUT_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (RU_OUT)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_RU_OUT_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_RU_OUT, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_RU_OUT_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_RU_OUT_Test_ROC <- roc(response = BTREE_RU_OUT_Test$BTREE_Observed,
             predictor = BTREE_RU_OUT_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_RU_OUT_Test$BTREE_Observed)))

(BTREE_RU_OUT_Test_ROCCurveAUC <- auc(BTREE_RU_OUT_Test_ROC)[1])

```

###  1.5.3 Random Oversampling - Outside Resampling (RO_OUT)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Random Oversampling- Outside Resampling ](https://dl.acm.org/doi/10.1145/1007730.1007735) performs a random replication of rows for the minority class instances prior to the cross-validation process to make the occurrence of levels with the majority class equal.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random oversampling before formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 50:50 (<span style="color: #FF0000">Class=M</span>:78, <span style="color: #FF0000">Class=R</span>:78)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 1.00000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V22</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V47</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.65801
|
```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Conducting random oversampling
##################################
set.seed(12345678)
PMA_PreModelling_Train_RO_OUT <- upSample(x = PMA_PreModelling_Train[,-ncol(PMA_PreModelling_Train)],
                                            y = PMA_PreModelling_Train$Class)
table(PMA_PreModelling_Train_RO_OUT$Class) 

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
set.seed(12345678)
BTREE_RO_OUT <- train(x = PMA_PreModelling_Train_RO_OUT[,!names(PMA_PreModelling_Train_RO_OUT) %in% c("Class")],
                   y = PMA_PreModelling_Train_RO_OUT$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_RO_OUT

BTREE_RO_OUT$finalModel

BTREE_RO_OUT$results

(BTREE_RO_OUT_Train_ROCCurveAUC <- BTREE_RO_OUT$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_RO_OUT_VarImp <- varImp(BTREE_RO_OUT, scale = TRUE)
plot(BTREE_RO_OUT_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (RO_OUT)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_RO_OUT_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_RO_OUT, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_RO_OUT_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_RO_OUT_Test_ROC <- roc(response = BTREE_RO_OUT_Test$BTREE_Observed,
             predictor = BTREE_RO_OUT_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_RO_OUT_Test$BTREE_Observed)))

(BTREE_RO_OUT_Test_ROCCurveAUC <- auc(BTREE_RO_OUT_Test_ROC)[1])

```

###  1.5.4 Synthetic Minority Oversampling Technique - Outside Resampling  (SMOTE_OUT)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Synthetic Minority Oversampling Technique - Outside Resampling ](https://dl.acm.org/doi/10.5555/1622407.1622416) generates new minority instances between existing instances prior to the cross-validation process. The new instances created are not just the copy of existing minority cases, instead the algorithm takes sample of feature space for each target class and its neighbors and then generates new instances that combine the features of the target cases with features of its neighbors.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied synthetic minority oversampling before formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 57:43 (<span style="color: #FF0000">Class=M</span>:72, <span style="color: #FF0000">Class=R</span>:54)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.99115
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V12</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.71645
|
```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Conducting synthetic minority oversampling technique
##################################
set.seed(12345678)
PMA_PreModelling_Train_SMOTE_OUT <- SMOTE(Class ~ ., 
                                          data  = PMA_PreModelling_Train)
table(PMA_PreModelling_Train_SMOTE_OUT$Class) 

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
set.seed(12345678)
BTREE_SMOTE_OUT <- train(x = PMA_PreModelling_Train_SMOTE_OUT[,!names(PMA_PreModelling_Train_SMOTE_OUT) %in% c("Class")],
                   y = PMA_PreModelling_Train_SMOTE_OUT$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_SMOTE_OUT

BTREE_SMOTE_OUT$finalModel

BTREE_SMOTE_OUT$results

(BTREE_SMOTE_OUT_Train_ROCCurveAUC <- BTREE_SMOTE_OUT$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_SMOTE_OUT_VarImp <- varImp(BTREE_SMOTE_OUT, scale = TRUE)
plot(BTREE_SMOTE_OUT_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (SMOTE_OUT)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_SMOTE_OUT_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_SMOTE_OUT, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_SMOTE_OUT_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_SMOTE_OUT_Test_ROC <- roc(response = BTREE_SMOTE_OUT_Test$BTREE_Observed,
             predictor = BTREE_SMOTE_OUT_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_SMOTE_OUT_Test$BTREE_Observed)))

(BTREE_SMOTE_OUT_Test_ROCCurveAUC <- auc(BTREE_SMOTE_OUT_Test_ROC)[1])

```

###  1.5.5 Random Oversampling Examples - Outside Resampling  (ROSE_OUT)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Random Oversampling Examples - Outside Resampling](https://link.springer.com/article/10.1007/s10618-012-0295-5) generates synthetic data by enlarging the feature space of minority and majority class instances prior to the cross-validation process. The algorithm draws new examples from a conditional kernel density estimate of the two classes. The kernel is a normal product function centered at each of the instances with diagonal covariance matrix defined with the width of the neighborhood which is the asymptotically optimal smoothing matrix under the assumption of multivariate normality. 
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random oversampling examples before formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 49:51 (<span style="color: #FF0000">Class=M</span>:47, <span style="color: #FF0000">Class=R</span>:49)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.86373
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V6</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V13</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V44</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V47</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.80087
|
```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Conducting random oversampling examples
##################################
set.seed(12345678)
PMA_PreModelling_Train_ROSE_OUT <- ROSE(Class ~ .,data  = PMA_PreModelling_Train)$data 
table(PMA_PreModelling_Train_ROSE_OUT$Class) 

names(PMA_PreModelling_Train_ROSE_OUT)


##################################
# Creating consistent fold assignments
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv",
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
set.seed(12345678)
BTREE_ROSE_OUT <- train(x = PMA_PreModelling_Train_ROSE_OUT[,!names(PMA_PreModelling_Train_ROSE_OUT) %in% c("Class")],
                   y = PMA_PreModelling_Train_ROSE_OUT$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_ROSE_OUT

BTREE_ROSE_OUT$finalModel

BTREE_ROSE_OUT$results

(BTREE_ROSE_OUT_Train_ROCCurveAUC <- BTREE_ROSE_OUT$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_ROSE_OUT_VarImp <- varImp(BTREE_ROSE_OUT, scale = TRUE)
plot(BTREE_ROSE_OUT_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (ROSE_OUT)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_ROSE_OUT_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_ROSE_OUT,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_ROSE_OUT_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_ROSE_OUT_Test_ROC <- roc(response = BTREE_ROSE_OUT_Test$BTREE_Observed,
             predictor = BTREE_ROSE_OUT_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_ROSE_OUT_Test$BTREE_Observed)))

(BTREE_ROSE_OUT_Test_ROCCurveAUC <- auc(BTREE_ROSE_OUT_Test_ROC)[1])

```

###  1.5.6 Random Undersampling - Inside Resampling (RU_IN)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance. 
|
| [Random Undersampling - Inside Resampling](https://dl.acm.org/doi/10.1145/1007730.1007735) performs a random removal of rows for the majority class instances within the cross-validation process to make the occurrence of levels with the minority class equal.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random undersampling while formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 50:50 (<span style="color: #FF0000">Class=M</span>:18, <span style="color: #FF0000">Class=R</span>:18)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.79089
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V51</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V58</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.88744
|
```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary,
                                   sampling = "down")

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
PMA_PreModelling_Train_RU_IN <- PMA_PreModelling_Train
set.seed(12345678)
BTREE_RU_IN <- train(x = PMA_PreModelling_Train_RU_IN[,!names(PMA_PreModelling_Train_RU_IN) %in% c("Class")],
                   y = PMA_PreModelling_Train_RU_IN$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_RU_IN

BTREE_RU_IN$finalModel

BTREE_RU_IN$results

(BTREE_RU_IN_Train_ROCCurveAUC <- BTREE_RU_IN$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_RU_IN_VarImp <- varImp(BTREE_RU_IN, scale = TRUE)
plot(BTREE_RU_IN_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (RU_IN)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_RU_IN_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_RU_IN, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_RU_IN_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_RU_IN_Test_ROC <- roc(response = BTREE_RU_IN_Test$BTREE_Observed,
             predictor = BTREE_RU_IN_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_RU_IN_Test$BTREE_Observed)))

(BTREE_RU_IN_Test_ROCCurveAUC <- auc(BTREE_RU_IN_Test_ROC)[1])

```

###  1.5.7 Random Oversampling - Inside Resampling (RO_IN)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Random Oversampling- Inside Resampling ](https://dl.acm.org/doi/10.1145/1007730.1007735) performs a random replication of rows for the minority class instances within the cross-validation process to make the occurrence of levels with the majority class equal.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random oversampling before formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 50:50 (<span style="color: #FF0000">Class=M</span>:78, <span style="color: #FF0000">Class=R</span>:78)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.86071
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V20</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.72727
|
```{r section_1.5.7, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary,
                                   sampling = "up")

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
PMA_PreModelling_Train_RO_IN <- PMA_PreModelling_Train
set.seed(12345678)
BTREE_RO_IN <- train(x = PMA_PreModelling_Train_RO_IN[,!names(PMA_PreModelling_Train_RO_IN) %in% c("Class")],
                   y = PMA_PreModelling_Train_RO_IN$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_RO_IN

BTREE_RO_IN$finalModel

BTREE_RO_IN$results

(BTREE_RO_IN_Train_ROCCurveAUC <- BTREE_RO_IN$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_RO_IN_VarImp <- varImp(BTREE_RO_IN, scale = TRUE)
plot(BTREE_RO_IN_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (RO_IN)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_RO_IN_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_RO_IN, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_RO_IN_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_RO_IN_Test_ROC <- roc(response = BTREE_RO_IN_Test$BTREE_Observed,
             predictor = BTREE_RO_IN_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_RO_IN_Test$BTREE_Observed)))

(BTREE_RO_IN_Test_ROCCurveAUC <- auc(BTREE_RO_IN_Test_ROC)[1])

```

###  1.5.8 Synthetic Minority Oversampling Technique - Inside Resampling (SMOTE_IN)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance.
|
| [Synthetic Minority Oversampling Technique - Inside Resampling](https://dl.acm.org/doi/10.5555/1622407.1622416) generates new minority instances between existing instances within the cross-validation process. The new instances created are not just the copy of existing minority cases, instead the algorithm takes sample of feature space for each target class and its neighbors and then generates new instances that combine the features of the target cases with features of its neighbors.
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied synthetic minority oversampling while formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 57:43 (<span style="color: #FF0000">Class=M</span>:72, <span style="color: #FF0000">Class=R</span>:54)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.80321
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V1</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V22</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V35</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V21</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.68831
|
```{r section_1.5.8, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv", 
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary,
                                   sampling = "smote")

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
PMA_PreModelling_Train_SMOTE_IN <- PMA_PreModelling_Train
set.seed(12345678)
BTREE_SMOTE_IN <- train(x = PMA_PreModelling_Train_SMOTE_IN[,!names(PMA_PreModelling_Train_SMOTE_IN) %in% c("Class")],
                   y = PMA_PreModelling_Train_SMOTE_IN$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_SMOTE_IN

BTREE_SMOTE_IN$finalModel

BTREE_SMOTE_IN$results

(BTREE_SMOTE_IN_Train_ROCCurveAUC <- BTREE_SMOTE_IN$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_SMOTE_IN_VarImp <- varImp(BTREE_SMOTE_IN, scale = TRUE)
plot(BTREE_SMOTE_IN_VarImp, 
     top=25, 
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (SMOTE_IN)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_SMOTE_IN_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_SMOTE_IN, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_SMOTE_IN_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_SMOTE_IN_Test_ROC <- roc(response = BTREE_SMOTE_IN_Test$BTREE_Observed,
             predictor = BTREE_SMOTE_IN_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_SMOTE_IN_Test$BTREE_Observed)))

(BTREE_SMOTE_IN_Test_ROCCurveAUC <- auc(BTREE_SMOTE_IN_Test_ROC)[1])

```

###  1.5.9 Random Oversampling Examples - Inside Resampling (ROSE_IN)
|
| [Bagged Trees](https://link.springer.com/article/10.1007/BF00058655) combine bootstrapping and decision trees to construct an ensemble. The modeling process involves generating bootstrap samples of the original data, training an unpruned decision tree for each bootstrap subset of the data and implementing an ensemble voting for all the individual decision tree predictions to formulate the final prediction. The bootstrap aggregation (bagging) mechanism improves the model performance by reducing variance. Although the individual decision trees in the model are identically distributed, they are not necessarily independent and share similar structure. This similarity, known as tree correlation, is an essential factor that prevents further reduction of variance. 
|
| [Random Oversampling Examples - Inside Resampling](https://link.springer.com/article/10.1007/s10618-012-0295-5) generates synthetic data by enlarging the feature space of minority and majority class instances within the cross-validation process. The algorithm draws new examples from a conditional kernel density estimate of the two classes. The kernel is a normal product function centered at each of the instances with diagonal covariance matrix defined with the width of the neighborhood which is the asymptotically optimal smoothing matrix under the assumption of multivariate normality. 
|
| **[A]** The bagged trees model from the <mark style="background-color: #CCECFF">**ipred**</mark>, <mark style="background-color: #CCECFF">**plyr**</mark> and <mark style="background-color: #CCECFF">**e1071**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. 
|
| **[B]** The model does not contain any hyperparameter.
|
| **[C]** This version of the data applied random oversampling examples while formulating the model as class imbalance treatment for the observations.
|      **[C.1]** Train Set = 36 observations with class ratio of 49:51 (<span style="color: #FF0000">Class=M</span>:47, <span style="color: #FF0000">Class=R</span>:49)
|      **[C.2]** Test Set = 40 observations with class ratio of 80:20 (<span style="color: #FF0000">Class=M</span>:33, <span style="color: #FF0000">Class=R</span>:7)
|      **[C.3]** 1 <span style="color: #FF0000">Class</span> response variable (factor)
|      **[C.4]** 60 predictor variables (numeric)
|
| **[D]** The cross-validated model performance of the final model on this data version is summarized as follows:
|      **[D.1]** Final model configuration is fixed due to the absence of a hyperparameter
|      **[D.2]** ROC Curve AUC = 0.79661
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V20</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V22</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V6</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V5</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** ROC Curve AUC = 0.69697
|
```{r section_1.5.9, warning=FALSE, message=FALSE}
##################################
# Creating consistent fold assignments
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
RepeatedCV_Control <- trainControl(method = "repeatedcv",
                                   repeats = 5,
                                   classProbs = TRUE,
                                   summaryFunction = twoClassSummary,
                                   sampling = "rose")

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
# No hyperparameter tuning process conducted

##################################
# Running the bagged trees model
# by setting the caret method to 'treebag'
##################################
PMA_PreModelling_Train_ROSE_IN <- PMA_PreModelling_Train
set.seed(12345678)
BTREE_ROSE_IN <- train(x = PMA_PreModelling_Train_ROSE_IN[,!names(PMA_PreModelling_Train_ROSE_IN) %in% c("Class")],
                   y = PMA_PreModelling_Train_ROSE_IN$Class,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = RepeatedCV_Control)

##################################
# Reporting the cross-validation results
# for the train set
##################################
BTREE_ROSE_IN

BTREE_ROSE_IN$finalModel

BTREE_ROSE_IN$results

(BTREE_ROSE_IN_Train_ROCCurveAUC <- BTREE_ROSE_IN$results$ROC)

##################################
# Identifying and plotting the
# best model predictors
##################################
BTREE_ROSE_IN_VarImp <- varImp(BTREE_ROSE_IN, scale = TRUE)
plot(BTREE_ROSE_IN_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Bagged Trees (ROSE_IN)",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
BTREE_ROSE_IN_Test <- data.frame(BTREE_Observed = PMA_PreModelling_Test$Class,
                      BTREE_Predicted = predict(BTREE_ROSE_IN,
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "prob"))

BTREE_ROSE_IN_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
BTREE_ROSE_IN_Test_ROC <- roc(response = BTREE_ROSE_IN_Test$BTREE_Observed,
             predictor = BTREE_ROSE_IN_Test$BTREE_Predicted.R,
             levels = rev(levels(BTREE_ROSE_IN_Test$BTREE_Observed)))

(BTREE_ROSE_IN_Test_ROCCurveAUC <- auc(BTREE_ROSE_IN_Test_ROC)[1])

```

## 1.6 Evaluation Summary
|
| Class imbalance treatment method comparison:
|
| **[A]** The class imbalance treatment methods which demonstrated the best and most consistent ROC Curve AUC metrics using the bagged trees model are as follows:
|      **[A.1]** RU-OUT: Random Undersampling - Outside Resampling
|             **[A.1.1]** Test ROC Curve AUC = 0.90909, Cross-Validation ROC Curve AUC = 0.82000
|      **[A.2]** RU-IN:  Random Undersampling - Inside Resampling
|             **[A.2.1]** Test ROC Curve AUC = 0.88744, Cross-Validation ROC Curve AUC = 0.79089
|      **[A.3]** ROSE-OUT: Random Oversampling Examples - Outside Resampling
|             **[A.3.1]** Test ROC Curve AUC = 0.80087, Cross-Validation ROC Curve AUC = 0.86373
|
```{r section_1.7, warning=FALSE, message=FALSE}
##################################
# Consolidating all evaluation results
# for the train and test sets
# using the ROC Curve AUC metric
##################################
Model <- c('REF','RU_OUT','RO_OUT','SMOTE_OUT','ROSE_OUT','RU_IN','RO_IN','SMOTE_IN','ROSE_IN',
           'REF','RU_OUT','RO_OUT','SMOTE_OUT','ROSE_OUT','RU_IN','RO_IN','SMOTE_IN','ROSE_IN')

Set <- c(rep('Cross-Validation',9),rep('Test',9))

ROCCurveAUC <- c(BTREE_REF_Train_ROCCurveAUC,
                 BTREE_RU_OUT_Train_ROCCurveAUC,
                 BTREE_RO_OUT_Train_ROCCurveAUC,
                 BTREE_SMOTE_OUT_Train_ROCCurveAUC,
                 BTREE_ROSE_OUT_Train_ROCCurveAUC,
                 BTREE_RU_IN_Train_ROCCurveAUC,
                 BTREE_RO_IN_Train_ROCCurveAUC,
                 BTREE_SMOTE_IN_Train_ROCCurveAUC,
                 BTREE_ROSE_IN_Train_ROCCurveAUC,
                 BTREE_REF_Test_ROCCurveAUC,
                 BTREE_RU_OUT_Test_ROCCurveAUC,
                 BTREE_RO_OUT_Test_ROCCurveAUC,
                 BTREE_SMOTE_OUT_Test_ROCCurveAUC,
                 BTREE_ROSE_OUT_Test_ROCCurveAUC,
                 BTREE_RU_IN_Test_ROCCurveAUC,
                 BTREE_RO_IN_Test_ROCCurveAUC,
                 BTREE_SMOTE_IN_Test_ROCCurveAUC,
                 BTREE_ROSE_IN_Test_ROCCurveAUC)

ROCCurveAUC_Summary <- as.data.frame(cbind(Model,Set,ROCCurveAUC))

ROCCurveAUC_Summary$ROCCurveAUC <- as.numeric(as.character(ROCCurveAUC_Summary$ROCCurveAUC))
ROCCurveAUC_Summary$Set <- factor(ROCCurveAUC_Summary$Set,
                                        levels = c("Cross-Validation",
                                                   "Test"))
ROCCurveAUC_Summary$Model <- factor(ROCCurveAUC_Summary$Model,
                                        levels =c('REF',
                                                  'RU_OUT',
                                                  'RO_OUT',
                                                  'SMOTE_OUT',
                                                  'ROSE_OUT',
                                                  'RU_IN',
                                                  'RO_IN',
                                                  'SMOTE_IN',
                                                  'ROSE_IN'))

print(ROCCurveAUC_Summary, row.names=FALSE)

(ROCCurveAUC_Plot <- dotplot(Model ~ ROCCurveAUC,
                           data = ROCCurveAUC_Summary,
                           groups = Set,
                           main = "Classification Model Performance Comparison",
                           ylab = "Model",
                           xlab = "ROC Curve AUC",
                           auto.key = list(adj = 1),
                           type=c("p", "h"),       
                           origin = 0,
                           alpha = 0.45,
                           pch = 16,
                           cex = 2))

##################################
# Consolidating the resampling results
# for the candidate models
##################################
(BTREE_RESAMPLING <- resamples(list(REF = BTREE_REF,
                                    RU_OUT = BTREE_RU_OUT,
                                    RO_OUT = BTREE_RO_OUT,
                                    SMOTE_OUT = BTREE_SMOTE_OUT,
                                    ROSE_OUT = BTREE_ROSE_OUT,
                                    RU_IN = BTREE_RU_IN,
                                    RO_IN = BTREE_RO_IN,
                                    SMOTE_IN = BTREE_SMOTE_IN,
                                    ROSE_IN = BTREE_ROSE_IN)))

summary(BTREE_RESAMPLING)

##################################
# Exploring the resampling results
##################################
bwplot(BTREE_RESAMPLING,
       main = "Model Resampling Performance Comparison (Range)",
       ylab = "Model",
       pch=16,
       cex=2,
       layout=c(3,1))

dotplot(BTREE_RESAMPLING,
       main = "Model Resampling Performance Comparison (95% Confidence Interval)",
       ylab = "Model",
       pch=16,
       cex=2,
       layout=c(3,1))

##################################
# Consolidating all models
##################################
(BTREE_MODELS <- (list(REF = BTREE_REF,
                            RU_OUT = BTREE_RU_OUT,
                            RO_OUT = BTREE_RO_OUT,
                            SMOTE_OUT = BTREE_SMOTE_OUT,
                            ROSE_OUT = BTREE_ROSE_OUT,
                            RU_IN = BTREE_RU_IN,
                            RO_IN = BTREE_RO_IN,
                            SMOTE_IN = BTREE_SMOTE_IN,
                            ROSE_IN = BTREE_ROSE_IN)))

##################################
# Creating a function model performance
# on test data
##################################
BTREE_TEST_ROCCurveAUC <- function(model, data) {
  ROCCurveAUC <- roc(data$Class, 
                 predict(model, data, type = "prob")[, "R"],
                 levels = c("M", "R"))
  ci(ROCCurveAUC)
}

BTREE_TEST_SUMMARY <- lapply(BTREE_MODELS, 
                             BTREE_TEST_ROCCurveAUC, 
                             data = PMA_PreModelling_Test)
BTREE_TEST_SUMMARY <- lapply(BTREE_TEST_SUMMARY, as.vector)
BTREE_TEST_SUMMARY <- do.call("rbind", BTREE_TEST_SUMMARY)
colnames(BTREE_TEST_SUMMARY) <- c("LCL", "ROC", "UCL")
(BTREE_TEST_SUMMARY <- as.data.frame(BTREE_TEST_SUMMARY))

```

# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[R Package]** [ROSE](https://cran.r-project.org/web/packages/ROSE/ROSE.pdf) by Nicola Lunardon
| **[R Package]** [themis](https://cran.r-project.org/web/packages/themis/themis.pdf) by Emil Hvitfeldt
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package – A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Bagging and Random Forest Ensemble Algorithms for Machine Learning](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/) by Jason Brownlee
| **[Article]** [A Brief Introduction to Bagged Trees, Random Forest and Their Applications in R](https://blog.zenggyu.com/en/post/2018-06-05/a-brief-introduction-to-bagged-trees-random-forest-and-their-applications-in-r/) by Nick Zeng
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [A Gentle Introduction to Imbalanced Classification](https://machinelearningmastery.com/what-is-imbalanced-classification/) by Jason Brownlee
| **[Article]** [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/#:~:text=8%20Tactics%20To%20Combat%20Imbalanced%20Training%20Data%201,Different%20Perspective%20...%208%208%29%20Try%20Getting%20Creative) by Jason Brownlee
| **[Article]** [Step-By-Step Framework for Imbalanced Classification Projects](https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/) by Jason Brownlee
| **[Article]** [How to Handle Imbalanced Classes in Machine Learning](https://elitedatascience.com/imbalanced-classes) by Elite Data Science Team
| **[Article]** [Best Ways To Handle Imbalanced Data in Machine Learning](https://dataaspirant.com/handle-imbalanced-data-machine-learning/) by Jaiganesh Nagidi
| **[Article]** [Handling Imbalanced Data for Classification](https://www.geeksforgeeks.org/handling-imbalanced-data-for-classification/) by Geeks For Geeks Team
| **[Article]** [Random Oversampling and Undersampling for Imbalanced Classification](https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/#:~:text=Random%20undersampling%20involves%20randomly%20selecting%20examples%20from%20the,45%2C%20Imbalanced%20Learning%3A%20Foundations%2C%20Algorithms%2C%20and%20Applications%2C%202013) by Jason Brownlee
| **[Publication]** [Bagging Predictors](https://link.springer.com/article/10.1007/BF00058655) by Leo Breiman (Machine Learning)
| **[Publication]** [SMOTE: Synthetic Minority Over-Sampling Technique](https://dl.acm.org/doi/10.5555/1622407.1622416) by Nitesh Chawla, Kevin Bowyer, Lawrence Hall and Philip Kegelmeyer (Journal of Artificial Intelligence Research)
| **[Publication]** [Training and Assessing Classification Rules with Imbalanced Data](https://link.springer.com/article/10.1007/s10618-012-0295-5) by Giovanna Menardi and Nicola Torelli (Data Mining and Knowledge Discovery)
| **[Publication]** [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](https://dl.acm.org/doi/10.1145/1007730.1007735) by Gustavo Batista, Ronaldo Prati and Maria Carolina Monard (ACM SIGKDD Explorations Newsletter)
| **[Publication]** [A Survey of Predictive Modelling under Imbalanced Distributions](https://arxiv.org/abs/1505.01658) by Paula Branco, Luis Torgo and Rita Ribeiro (Arxiv Cornell University)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|